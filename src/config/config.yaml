# Universal Training Configuration
# Dynamic, modular config for cattle detection training
# Auto-detects: num_classes, class_names, image/label counts at runtime

# ============================================================================
# ACTIVE PRESET - Change this to switch training modes quickly
# ============================================================================
active_preset: standard # Options: quick_test, standard, high_performance, custom

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  name: cattlebody # Options: cattle, cattlebody, cattleface
  split: raw # Options: raw, processed
  root: dataset/${dataset.name} # Auto-constructed path
  format: auto # auto-detect format (yolo, coco, voc)

  # Dataset properties detected at runtime - DO NOT SET MANUALLY
  # num_classes: auto-detected
  # class_names: auto-detected
  # splits: auto-detected

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  name: fusion_model  # Using the fusion model for better performance
  pretrained: true  # Use pretrained weights
  freeze_backbone: false  # Allow full model training for better adaptation
  
  # Feature Pyramid Network settings
  fpn:
    use_p2: true  # Enable P2 level for better small object detection
    use_p6_p7: true  # Additional levels for large object detection
    channels: [256, 256, 256, 256, 256]  # Consistent channel size

  # Anchor settings
  anchors:
    sizes: [[16, 24, 32], [48, 64, 96], [128, 192, 256]]
    aspect_ratios: [0.5, 0.75, 1.0, 1.5, 2.0]
    scales: [1.0, 1.25, 1.5]

# ============================================================================
# VGG16-YOLOv8 CONFIGURATION
# ============================================================================
vgg16_yolov8:
  # Model architecture
  backbone:
    pretrained: true  # Use pretrained VGG16 weights
    freeze_layers: false  # Allow full backbone training
    output_indices: [3, 4]  # Use conv4 and conv5 features
    bn_eps: 1e-5
    bn_momentum: 0.1

  # Detection head
  head:
    num_classes: 2  # Number of classes to detect
    anchors_per_cell: 3  # Number of anchors per grid cell
    strides: [8, 16, 32]  # Feature map strides
    in_channels: [512, 1024, 2048]  # Input channels for each detection level

  # Training specifics
  training:
    iou_threshold: 0.5  # IoU threshold for positive samples
    ignore_threshold: 0.4  # IoU threshold for ignoring anchors
    anchor_t: 4.0  # Anchor threshold
    box_loss_weight: 0.05  # Box regression loss weight
    cls_loss_weight: 0.5  # Classification loss weight
    obj_loss_weight: 1.0  # Objectness loss weight

  # Inference settings
  inference:
    conf_threshold: 0.25  # Confidence threshold for detections
    nms_threshold: 0.45  # NMS IoU threshold
    max_detections: 300  # Maximum detections per image

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================
train:
  epochs: 200  # Train longer for better convergence
  batch_size: 8  # Smaller batch size for better generalization
  learning_rate: 0.0005  # Balanced learning rate
  warmup_epochs: 5  # Gradual warmup
  weight_decay: 0.0005
  momentum: 0.937

  # Optimizer
  optimizer: adamw
  optimizer_params:
    betas: [0.937, 0.999]
    eps: 1.0e-8

  # Loss configuration
  loss:
    type: focal  # Use focal loss for better handling of class imbalance
    focal_alpha: 0.25
    focal_gamma: 2.0
    box_loss_weight: 1.0
    cls_loss_weight: 1.0
    obj_loss_weight: 1.0

# ============================================================================
# TRANSFORMS AND DATA AUGMENTATION
# ============================================================================
transforms:
  # Common Settings
  input_size: 640  # Target image size for model input
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

  # Training Augmentations
  train:
    enabled: true
    mosaic:
      enabled: true
      probability: 0.3
      min_scale: 0.3
      max_scale: 0.7
    random_crop:
      enabled: true
      scale_range: [0.8, 1.0]
    geometry:
      horizontal_flip: 0.5
      vertical_flip: 0.2
      rotate90: 0.2
    color:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    noise:
      gaussian: 0.2
      motion_blur: 0.2
    enhancement:
      clahe: 0.3
      sharpen: 0.3
    box_validation:
      min_area: 256  # 16*16 pixels - reduced to keep smaller objects
      min_visibility: 0.2  # Reduced to handle more partial occlusions
      
  # Validation/Test Transforms
  val:
    enabled: true
    input_size: 640  # Keep consistent with training size
    normalize: true
    multi_scale: true  # Enable multi-scale validation
    conf_threshold: 0.05  # Lower confidence threshold for better recall
    nms_iou_threshold: 0.5  # Balanced NMS threshold
    max_detections: 500  # Increased max detections
    use_soft_nms: true  # Enable soft-NMS for better overlapping detections

preprocess:
  enabled: true
  target_size: [1280, 1280]  # Increased size for better small object detection
  maintain_aspect: true  # Use letterboxing to maintain aspect ratio
  normalize: true  # Normalize to [0, 1]
  format: yolo  # Output format for preprocessed data
  min_object_area: 25  # Minimum object area in pixels (5x5)
  max_objects: 300  # Increased maximum objects per image

  # Image quality filters
  min_image_size: [320, 320] # Filter out images smaller than this (detection minimum)
  max_image_size: null # Filter out images larger than this (null = no limit)

  # Label quality filters
  min_bbox_size: 0.001 # Minimum bbox area (normalized)
  filter_invalid_boxes: true # Remove boxes outside image bounds

# ============================================================================
# DATA AUGMENTATION (Applied during training)
# ============================================================================
augmentation:
  enabled: true

  # Geometric augmentations
  horizontal_flip: 0.5
  vertical_flip: 0.0
  rotation: 10 # degrees
  scale: [0.8, 1.2]
  translate: 0.1 # fraction of image
  shear: 0.0 # degrees

  # Color augmentations
  brightness: 0.2 # +/- 20%
  contrast: 0.2
  saturation: 0.2
  hue: 0.1

  # Advanced augmentations
  mosaic: true # Mosaic augmentation (4 images combined)
  mixup: 0.0 # Mixup alpha (0 = disabled)
  cutout: 0.0 # Cutout probability (0 = disabled)

# ============================================================================
# REGULARIZATION
# ============================================================================
regularization:
  dropout: 0.3
  label_smoothing: 0.0
  gradient_clip: 1.0
  weight_decay: 0.0001

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: weighted # Changed to weighted to handle class imbalance
  # Focal loss params (for class imbalance)
  focal_alpha: 0.5  # Increased to give more weight to positive samples
  focal_gamma: 1.5  # Reduced to make loss less harsh on hard examples
  # Weighted loss (auto-computed from dataset if type=weighted)
  class_weights: auto
  box_loss_weight: 5.0  # Increased weight on bounding box regression
  objectness_weight: 1.0  # Weight for objectness predictions

# ============================================================================
# TRAINING STRATEGY
# ============================================================================
strategy:
  warmup_epochs: 3  # Reduced warmup for faster adaptation
  warmup_momentum: 0.8  # Momentum during warmup
  warmup_bias_lr: 0.1  # Separate learning rate for bias terms during warmup
  scheduler: cosine # Options: cosine, step, multistep, plateau
  early_stopping: true
  patience: 15  # Reduced patience for faster response to stagnation
  mixed_precision: true # Use AMP for faster training
  ema: true  # Enable EMA for model weights
  ema_decay: 0.9999  # EMA decay rate
  anchor_t: 4.0  # Anchor-point threshold
  anchor_update_interval: 100  # Update anchors every N iterations
  anchor_update_epochs: 1  # Update anchors for first N epochs

# ============================================================================
# VALIDATION
# ============================================================================
validation:
  interval: 1 # Validate every N epochs
  metric: mAP # Primary metric: mAP, loss, f1
  iou_threshold: 0.3 # IoU threshold for mAP calculation (temporarily reduced for early training)
  conf_threshold: 0.05  # Lowered to detect more potential objects
  nms_iou_threshold: 0.65  # Increased to preserve more overlapping detections
  multi_scale: true  # Enable multi-scale validation
  use_soft_nms: true  # Use soft-NMS for better recall

# ============================================================================
# CHECKPOINTING
# ============================================================================
checkpoint:
  save_interval: 5 # Save checkpoint every N epochs
  save_best_only: false # Save only best model (by validation metric)
  save_last: true # Always save last checkpoint
  resume: null # Path to checkpoint to resume from

# ============================================================================
# OUTPUT & LOGGING
# ============================================================================
output:
  base_dir: outputs # Base output directory
  experiment_name: null # Auto-generated: {dataset}_{model}_{timestamp}
  log_interval: 10 # Log every N batches
  save_predictions: true
  save_visualizations: true
  tensorboard: true # Enable tensorboard logging

# ============================================================================
# VISUALIZATION
# ============================================================================
visualization:
  enabled: true
  save_interval: 10 # Save visualizations every N epochs
  num_samples: 8 # Number of samples to visualize
  show_ground_truth: true
  show_predictions: true
  confidence_threshold: 0.5

# ============================================================================
# DEVICE & COMPUTE
# ============================================================================
device:
  type: cuda # Options: cuda, cpu, mps
  gpu_ids: [0] # GPU IDs to use (for multi-GPU)
  workers: 4 # Dataloader workers
  pin_memory: true

# ============================================================================
# DEBUGGING & DEVELOPMENT
# ============================================================================
debug:
  enabled: false
  profile: false # Profile training performance
  fast_dev_run: false # Run 1 batch for debugging
  seed: 42 # Random seed for reproducibility

# ============================================================================
# TRAINING PRESETS (Override settings above based on use case)
# ============================================================================
presets:
  quick_test:
    # Fast experimentation - minimal epochs, smaller size
    train:
      epochs: 20
      batch_size: 16
      learning_rate: 0.001
    preprocess:
      target_size: [416, 416]
    augmentation:
      rotation: 5
      brightness: 0.1
      contrast: 0.1
    strategy:
      early_stopping: false
      warmup_epochs: 2
    checkpoint:
      save_interval: 10
      save_best_only: true
    output:
      save_predictions: false
      save_visualizations: false
    validation:
      interval: 2

  standard:
    # Balanced training - good results with reasonable time
    train:
      epochs: 100
      batch_size: 8
      learning_rate: 0.001
    preprocess:
      target_size: [640, 640]
    augmentation:
      rotation: 10
      brightness: 0.2
      contrast: 0.2
      mosaic: true
    strategy:
      early_stopping: true
      patience: 20
      warmup_epochs: 5
    checkpoint:
      save_interval: 5
      save_best_only: false
    validation:
      interval: 1

  high_performance:
    # Maximum accuracy - long training, aggressive augmentation
    train:
      epochs: 300
      batch_size: 16
      learning_rate: 0.002
    preprocess:
      target_size: [640, 640] # Can increase to 1280 for small objects
    augmentation:
      enabled: true
      horizontal_flip: 0.5
      vertical_flip: 0.2
      rotation: 15
      brightness: 0.3
      contrast: 0.3
      saturation: 0.3
      hue: 0.15
      mosaic: true
      mixup: 0.2
    regularization:
      label_smoothing: 0.1
    strategy:
      early_stopping: true
      patience: 50
      warmup_epochs: 10
    checkpoint:
      save_interval: 10
    validation:
      interval: 1

  custom:
    # Use the base configuration defined above
    # No overrides - fully customizable via config.yaml
# ============================================================================
# DATASET-SPECIFIC PROFILES
# ============================================================================
# Auto-filled from deep analysis, enhanced with preprocessing strategies
# These are automatically loaded from dataset_analysis_results/ at runtime

dataset_profiles:
  # --------------------------------------------------------------------------
  # CATTLEBODY - Single object body detection
  # --------------------------------------------------------------------------
  cattlebody:
    modality: rgb_camera
    task: single_object_detection

    analysis:
      num_classes: 1
      class_names: ["Cattlebody"]
      format: yolo
      has_data_yaml: true

      # Statistics from analysis
      image_stats:
        mean: [0.5380, 0.5380, 0.5380]
        std: [0.0820, 0.0820, 0.0820]
        brightness_mean: 137.20
        contrast_mean: 60.69
        aspect_ratio_range: [0.75, 2.19]

      object_stats:
        objects_per_image: 1.009
        avg_bbox_area: 0.105
        bbox_aspect_ratio: 0.985

      quality_issues:
        - "train: Image/label mismatch (3424 images, 3432 labels)"
        - "Large aspect ratio variation (0.75 to 2.19)"

      recommendations:
        - "Resize to 640x640 with letterboxing"
        - "Use standard loss (no class imbalance)"
        - "Fix train split mismatch via preprocessing"

    # Optimized settings for this dataset
    override:
      preprocess:
        target_size: [640, 640]
        maintain_aspect: true
        fix_mismatches: true
      train:
        batch_size: 16
        learning_rate: 0.001
      loss:
        type: standard

  # --------------------------------------------------------------------------
  # CATTLE - Multi-object detection with class imbalance
  # --------------------------------------------------------------------------
  cattle:
    modality: rgb_camera
    task: multi_object_detection

    analysis:
      num_classes: 2
      class_names: ["class_0", "class_1"]
      format: unknown
      has_data_yaml: false

      # Statistics
      image_stats:
        mean: [0.4272, 0.4272, 0.4272]
        std: [0.1097, 0.1097, 0.1097]
        brightness_mean: 108.96
        contrast_mean: 65.25
        aspect_ratio_range: [0.75, 1.91]

      object_stats:
        objects_per_image: 4.907
        avg_bbox_area: 0.029
        bbox_aspect_ratio: 0.570
        class_imbalance_ratio: 10.40 # SEVERE imbalance!

      quality_issues:
        - "Class imbalance detected (ratio: 10.40)"
        - "Small objects (avg area: 0.029)"

      recommendations:
        - "Use higher resolution (1280x1280) for small objects"
        - "Use focal loss for class imbalance"
        - "Aggressive augmentation (mosaic, mixup)"

    # Optimized settings for this dataset
    override:
      preprocess:
        target_size: [1280, 1280] # Higher for small objects
        min_bbox_size: 0.0005
        normalization:
          method: "dataset_specific"
          mean: [0.4272, 0.4272, 0.4272]
          std: [0.1097, 0.1097, 0.1097]
      augmentation:
        vertical_flip: 0.1
        rotation: 15
        scale: [0.7, 1.3]
        mixup: 0.15
      train:
        epochs: 150
        batch_size: 8 # Smaller due to 1280 resolution
        learning_rate: 0.0005
      loss:
        type: focal
        focal_alpha: 0.25
        focal_gamma: 2.0
      strategy:
        warmup_epochs: 10
        patience: 30

  # --------------------------------------------------------------------------
  # CATTLEFACE - BROKEN (no labels)
  # --------------------------------------------------------------------------
  cattleface:
    modality: rgb_camera
    task: face_detection
    status: broken # Cannot use for training

    analysis:
      num_classes: 0
      class_names: []
      quality_issues:
        - "NO LABELS FOUND - Cannot train!"
        - "All splits missing annotations"

      recommendations:
        - "Find original annotation files or re-annotate"

    # Cannot configure training without labels
    override:
      enabled: false
